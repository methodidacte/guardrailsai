{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/v0.1/docs/integrations/chat/azure_chat_openai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# If you're running this inside a notebook, you also need to patch the AsyncIO loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"***\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://***.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt432k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = HumanMessage(\n",
    "    content=\"Translate this sentence from English to French. I love programming.\"\n",
    ")\n",
    "model.invoke([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant that translates English to French.\"),\n",
    "    (\"human\", \"Translate this sentence from English to French. I love programming.\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    ")\n",
    "\n",
    "prompt = \"Quelle est la version de la librairie OpenAI ?\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "        model=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt content filter result in \"model_extra\" for azure\n",
    "prompt_filter_result = completion.model_extra[\"prompt_filter_results\"][0][\"content_filter_results\"]\n",
    "print(\"\\nPrompt content filter results:\")\n",
    "for category, details in prompt_filter_result.items():\n",
    "    print(f\"{category}:\\n filtered={details['filtered']}\\n severity={details['severity']}\")\n",
    "\n",
    "# completion content filter result\n",
    "print(\"\\nCompletion content filter results:\")\n",
    "completion_filter_result = completion.choices[0].model_extra[\"content_filter_results\"]\n",
    "for category, details in completion_filter_result.items():\n",
    "    print(f\"{category}:\\n filtered={details['filtered']}\\n severity={details['severity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardrails AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install guardrails-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip freeze | grep guardrails-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@krshranjith/how-to-use-guardrail-to-langchain-c3d5312d431b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import guardrails as gd\n",
    "from rich import print\n",
    "import pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rail = \"\"\"\n",
    "<rail version=\"0.1\">\n",
    "<output>\n",
    "<object description=\"classified label\" name=\"labels\"></object>\n",
    "<object format=\"valid-range: 0 1\" name=\"scores\"></object>\n",
    "</output>\n",
    "<prompt>\n",
    "\n",
    "Given the following llm output about a classification label and scores, please extract a dictionary that contains the label and scores.\n",
    "\n",
    "${llm_output}\n",
    "\n",
    "${gr.complete_json_suffix_v2}\n",
    "</prompt>\n",
    "</rail>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guardrails as gr\n",
    "\n",
    "guard = gr.Guard.from_rail_string(rail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Marie, après avoir obtenu son baccalauréat au Lycée Henri-IV à Paris, a poursuivi ses études à l'Université Paris-Sorbonne où elle a obtenu une licence en lettres modernes. Passionnée par la littérature, elle a ensuite intégré Hachette Livre en tant qu'éditrice junior. Aujourd'hui, elle est rédactrice en chef dans cette même maison d'édition.\n",
    "\n",
    "Pierre, quant à lui, a suivi une formation en ingénierie informatique à l'École Polytechnique. Après son diplôme, il a été recruté par Capgemini en tant que développeur logiciel. Grâce à son talent et à son travail acharné, il occupe désormais le poste de directeur de projet chez Dassault Systèmes.\n",
    "\n",
    "Sophie a commencé ses études au Lycée Louis-le-Grand avant de rejoindre Sciences Po Paris. Avec un master en relations internationales en poche, elle a commencé sa carrière au sein du ministère des Affaires étrangères. Elle travaille maintenant comme diplomate à l'ambassade de France à Washington.\n",
    "\n",
    "Enfin, Jean, après un cursus au Lycée Saint-Louis à Paris, a intégré l'ESSEC Business School. Spécialisé en finance, il a débuté chez BNP Paribas en tant qu'analyste financier. Aujourd'hui, il est directeur financier chez L'Oréal, où il supervise les investissements et la stratégie financière de la société.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guardrails as gd\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "from pydantic import Field\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    school: Optional[str] = Field(..., description=\"The school this person attended\")\n",
    "    company: Optional[str] = Field(..., description=\"The company this person works for\")\n",
    "\n",
    "class People(BaseModel):\n",
    "    people: List[Person]\n",
    "\n",
    "# class ExtractedInfo(BaseModel):\n",
    "#     people: List[Person]\n",
    "#     companies: List[Company]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard = gd.Guard.from_pydantic(output_class=People, prompt=\"Get the following objects from the text:\\n\\n ${text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard = gd.Guard.from_pydantic(output_class=People, prompt=\"Get the following objects from the text:\\n\\n Jean, après un cursus au Lycée Saint-Louis à Paris, a intégré l'ESSEC Business School. Spécialisé en finance, il a débuté chez BNP Paribas en tant qu'analyste financier. Aujourd'hui, il est directeur financier chez L'Oréal, où il supervise les investissements et la stratégie financière de la société.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guardrails as gd\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "from pydantic import Field\n",
    "\n",
    "class Sinistre(BaseModel):\n",
    "    objet: str\n",
    "    date: Optional[str] = Field(..., description=\"date à laquelle s'est produit le sinistre\")\n",
    "    degat: Optional[str] = Field(..., description=\"dégât provoqué par le sinistre\")\n",
    "    cause: Optional[str] = Field(..., description=\"cause des dégâts\")\n",
    "\n",
    "class Sinistres(BaseModel):\n",
    "    sinistres: List[Sinistre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard = gd.Guard.from_pydantic(output_class=People, prompt=\"Get the following objects from the text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Je vous écris pour déclarer un sinistre concernant un dégât des eaux survenu dans mon domicile le 25 mai 2024. En rentrant chez moi ce soir-là, j’ai découvert que ma salle de bain était inondée. L'eau avait débordé de la baignoire en raison d’un problème de plomberie, et s'était répandue dans le couloir et la chambre adjacente.\n",
    "\n",
    "Les dégâts sont assez importants : le parquet du couloir et de la chambre est complètement imbibé d’eau, et la peinture des murs commence déjà à cloquer. Plusieurs meubles, notamment une commode en bois et un tapis persan, ont été endommagés par l'humidité. J’ai immédiatement coupé l’eau et contacté un plombier d’urgence pour stopper la fuite.\n",
    "\n",
    "Je vous joins à ce courrier des photos des dégâts ainsi que le devis du plombier pour la réparation de la plomberie défectueuse. Je souhaiterais savoir quelles sont les démarches à suivre pour que vous puissiez prendre en charge les frais de réparation et de remplacement des biens endommagés.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard = gd.Guard.from_pydantic(output_class=Sinistres, prompt=\"Get the following objects from the text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_llm_output, validated_output, *rest = guard(\n",
    "#     client.chat.completions.create(\n",
    "#         model=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "#         messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "#     ),\n",
    "#     engine=os.environ.get(\"AZURE_OPENAI_API_ENGINE\"),\n",
    "#     max_tokens=1024,\n",
    "#     temperature=0.3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Jean, après un cursus au Lycée Saint-Louis à Paris, a intégré l'ESSEC Business School. Spécialisé en finance, il a débuté chez BNP Paribas en tant qu'analyste financier. Aujourd'hui, il est directeur financier chez L'Oréal, où il supervise les investissements et la stratégie financière de la société.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "try:\n",
    "    validated_response = guard(\n",
    "        litellm.completion,\n",
    "        model=\"azure/gpt-4o\", # os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"]\n",
    "        max_tokens=500,\n",
    "        api_base=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "        api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "        api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "        msg_history=[{\"role\": \"user\", \"content\": \"{}\".format(text)}],\n",
    "        prompt_params={\"text\": text},\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validated_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LiteLLM is a lightweight wrapper that unifies the interface for over 100+ LLMs. Guardrails only supports 4 LLMs natively, but you can use Guardrails with LiteLLM to support over 100+ LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!guardrails configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!guardrails hub install hub://guardrails/regex_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guardrails import Guard, OnFailAction\n",
    "from guardrails.hub import RegexMatch\n",
    "\n",
    "guard = Guard().use(\n",
    "    RegexMatch, regex=\"\\(?\\d{3}\\)?-? *\\d{3}-? *-?\\d{4}\", on_fail=OnFailAction.EXCEPTION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard.validate(\"123-456-7890\")  # Guardrail passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    guard.validate(\"1234-789-0000\")  # Guardrail fails\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guard.history.last.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!guardrails hub install hub://guardrails/profanity_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "\n",
    "# def find_module(method_name):\n",
    "#   for module in sys.modules:\n",
    "#     print(\"checking module: \", module)\n",
    "#     if hasattr(sys.modules[module], method_name):\n",
    "#       return module\n",
    "#     # return None\n",
    "\n",
    "# module_name = find_module(\"ProfanityFree\")\n",
    "\n",
    "# print(f\"The ProfanityFree method is located in the {module_name} library.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guardrails.hub as hub\n",
    "\n",
    "\n",
    "hub_init = hub.__file__\n",
    "\n",
    "print(\"guardrails hub init is located at: \", hub_init)\n",
    "\n",
    "with open(hub_init) as hub_init_file:\n",
    "  print(\"\\n\")\n",
    "  print(hub_init_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!guardrails hub install hub://guardrails/competitor_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!guardrails hub install hub://guardrails/toxic_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import litellm\n",
    "from guardrails import Guard\n",
    "from guardrails.hub import ProfanityFree\n",
    "\n",
    "# litellm.set_verbose=True\n",
    "\n",
    "# Create a Guard class\n",
    "guard = Guard().use(ProfanityFree())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validated_response = guard(\n",
    "    litellm.completion,\n",
    "    model=\"azure/gpt-4o\", # os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"]\n",
    "    max_tokens=500,\n",
    "    api_base=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    msg_history=[{\"role\": \"user\", \"content\": \"hello\"}],\n",
    "    # prompt_params={\"text\": text},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guardrails import Guard, OnFailAction\n",
    "from guardrails.hub import CompetitorCheck, ToxicLanguage\n",
    "\n",
    "guard = Guard().use_many(\n",
    "    CompetitorCheck([\"X_company\", \"Y_company\", \"Z_company\"], on_fail=\"fix\"),\n",
    "    ToxicLanguage(threshold=0.5, validation_method=\"sentence\", on_fail=OnFailAction.EXCEPTION),\n",
    ")\n",
    "\n",
    "try:\n",
    "    output = guard.validate(\n",
    "        \"\"\"La société X_company est le pire assureur du marché. A éviter !\"\"\"\n",
    "    )  # Both the guardrails fail\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    validated_response = guard(\n",
    "        litellm.completion,\n",
    "        model=\"azure/gpt-4o\", # os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"]\n",
    "        max_tokens=500,\n",
    "        api_base=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "        api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "        api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "        msg_history=[{\"role\": \"user\", \"content\": \"Quelles sont les grandes compagnies d'assurance ?\"}],\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validated_response.raw_llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validated_response.validated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "\n",
    "distance = Levenshtein.distance(validated_response.raw_llm_output, validated_response.validated_output)\n",
    "print(f\"La distance de Levenshtein entre raw et validated est de {distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guard.history.last.tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
